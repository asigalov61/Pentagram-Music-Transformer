{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGrGd6__l5ch"
      },
      "source": [
        "# Pentagram Music Transformer Maker (ver. 1.0)\n",
        "\n",
        "***\n",
        "\n",
        "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
        "\n",
        "***\n",
        "\n",
        "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2023\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shLrgoXdl5cj"
      },
      "source": [
        "# GPU check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3rABEpKCO02"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RcVC4btl5ck"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viHgEaNACPTs"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/asigalov61/tegridy-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK40g6V_BTNj"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch==2.0.1\n",
        "!pip install torch-summary\n",
        "#!pip install sklearn\n",
        "!pip install tqdm\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzCOZU_gBiQV"
      },
      "outputs": [],
      "source": [
        "# Load modules and make data dir\n",
        "\n",
        "print('Loading modules...')\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import secrets\n",
        "import tqdm\n",
        "import time\n",
        "import math\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchsummary import summary\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools/nanoGPT\n",
        "\n",
        "from nanoGPT import GPTConfig, GPT\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools/\n",
        "\n",
        "import TMIDIX\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "if not os.path.exists('/content/INTS'):\n",
        "    os.makedirs('/content/INTS')\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbhzy8FGl5cm"
      },
      "source": [
        "# Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdBpL-HUHLBW"
      },
      "outputs": [],
      "source": [
        "CHUNKS_LENGTH = 4096+4 # must be larger than model seq_len and must be divisible by 5\n",
        "MIN_NUMBER_OF_CHUNK_EVENTS = 515 # min number of tokens per chunk\n",
        "PAD_IDX = 1670 # model pad index\n",
        "\n",
        "dataset_addr = \"/content/INTS\"\n",
        "\n",
        "filez = list()\n",
        "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
        "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
        "print('=' * 70)\n",
        "\n",
        "filez.sort()\n",
        "\n",
        "print('Loading training data... Please wait...')\n",
        "print('=' * 70)\n",
        "\n",
        "train_data = []\n",
        "\n",
        "chunks_counter = 0\n",
        "discarted_chunks_counter = 0\n",
        "\n",
        "for f in tqdm.tqdm(filez):\n",
        "  train_d = pickle.load(open(f, 'rb'))\n",
        "  random.shuffle(train_d)\n",
        "  for t in train_d:\n",
        "\n",
        "    for i in range(0, len(t), CHUNKS_LENGTH):\n",
        "\n",
        "      if 0 <= max(t[i:i+CHUNKS_LENGTH]) < PAD_IDX: # final data integrity check\n",
        "\n",
        "        if len(t[i:i+CHUNKS_LENGTH]) == CHUNKS_LENGTH:\n",
        "          train_data.append(t[i:i+CHUNKS_LENGTH])\n",
        "\n",
        "        else:\n",
        "          if len(t[i:i+CHUNKS_LENGTH]) > MIN_NUMBER_OF_CHUNK_EVENTS:\n",
        "            td = t[i:i+CHUNKS_LENGTH] + [PAD_IDX] * (CHUNKS_LENGTH-len(t[i:i+CHUNKS_LENGTH])) # padding with pad index\n",
        "            train_data.append(td)\n",
        "          else:\n",
        "            discarted_chunks_counter += 1\n",
        "\n",
        "        chunks_counter += 1\n",
        "\n",
        "print('=' * 70)\n",
        "print('Total number of imput chunks:', chunks_counter)\n",
        "print('Total number of good chunks:', len(train_data))\n",
        "print('Total number of discarted chunks:', discarted_chunks_counter, '/', round(100 * discarted_chunks_counter/chunks_counter, 3), '%')\n",
        "print('All data is good:', len(max(train_data, key=len)) == len(min(train_data, key=len)))\n",
        "print('=' * 70)\n",
        "print('Final data randomization...')\n",
        "random.shuffle(train_data)\n",
        "print('Done!')\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtHSA8MeCpGU"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLg-NI2BCrtQ"
      },
      "outputs": [],
      "source": [
        "train_data[0][:10], train_data[0][-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrawdn_ESX9p"
      },
      "outputs": [],
      "source": [
        "len(train_data) // 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhZqBvqVl5cn"
      },
      "source": [
        "# Setup model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJPxxFiwl5cn"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HETGqz_6K1ml"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "SEQ_LEN = 4096\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 1\n",
        "GRADIENT_ACCUMULATE_EVERY = 4\n",
        "\n",
        "NUM_BATCHES = (len(train_data) // BATCH_SIZE // GRADIENT_ACCUMULATE_EVERY) * NUM_EPOCHS\n",
        "\n",
        "SAVE_EVERY = 200\n",
        "PRINT_STATS_EVERY = 5\n",
        "#VALIDATE_EVERY  = 100\n",
        "#GENERATE_EVERY  = 200\n",
        "#GENERATE_LENGTH = 32\n",
        "\n",
        "eval_iters = 4\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "\n",
        "out_dir = '/content/OUT'\n",
        "\n",
        "# model\n",
        "\n",
        "n_layer = 32\n",
        "n_head = 32\n",
        "n_embd = 1024\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 2e-4 # max learning rate\n",
        "\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "\n",
        "\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "eval_interval = SAVE_EVERY\n",
        "log_interval = PRINT_STATS_EVERY\n",
        "\n",
        "# data\n",
        "\n",
        "gradient_accumulation_steps = GRADIENT_ACCUMULATE_EVERY # used to simulate larger batch sizes\n",
        "batch_size = BATCH_SIZE # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = SEQ_LEN\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "\n",
        "# poor man's data loader\n",
        "def get_batch(train_data, index):\n",
        "\n",
        "    ix = train_data[index*batch_size:(index*batch_size)+batch_size]\n",
        "    x = torch.stack([torch.LongTensor(i[:block_size]) for i in ix])\n",
        "    y = torch.stack([torch.LongTensor(i[1:block_size+1]) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout, ignore_idx=PAD_IDX) # start with model_args from command line\n",
        "\n",
        "\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "\n",
        "    model_args['vocab_size'] = PAD_IDX+1\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "\n",
        "optimizer = model.module.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    out1 = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        accs = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            bidx = random.randint(0, NUM_BATCHES)\n",
        "            X, Y = get_batch(train_data, bidx)\n",
        "            with ctx:\n",
        "                logits, loss, acc = model(X, Y, compute_acc=True)\n",
        "            losses[k] = loss.mean().item()\n",
        "            accs[k] = acc.mean().item()\n",
        "        out[split] = losses.mean()\n",
        "        out1[split] = accs.mean()\n",
        "    model.train()\n",
        "    return out, out1\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch(train_data, 0) # fetch the very first batch\n",
        "batch_num = 1\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "running_mfu = -1.0\n",
        "for i in tqdm.tqdm(range(NUM_BATCHES)):\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses, accs = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"step {iter_num}: train acc {accs['train']:.4f}, val acc {accs['val']:.4f}\")\n",
        "\n",
        "        val_losses.append(losses['val'].item())\n",
        "        val_accs.append(accs['val'].item())\n",
        "\n",
        "\n",
        "        best_val_loss = losses['val']\n",
        "        best_val_acc = accs['val']\n",
        "        if iter_num > 0:\n",
        "            checkpoint = {\n",
        "                'model': model.state_dict(),\n",
        "                'model_args': model_args,\n",
        "                'iter_num': iter_num,\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'train/loss': losses['train'],\n",
        "                'val/loss': losses['val'],\n",
        "                'train/acc': accs['train'],\n",
        "                'val/acc': accs['val'],\n",
        "                'lr': learning_rate,\n",
        "                'config': config,\n",
        "            }\n",
        "            print(f\"saving checkpoint to {out_dir}\")\n",
        "            torch.save(checkpoint, os.path.join(out_dir, 'model_checkpoint_'+str(iter_num)+'_steps_'+str(best_val_loss.item())+'_loss_'+str(best_val_acc.item())+'_acc.pth'))\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss, acc = model(X, Y, compute_acc=True)\n",
        "\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch(train_data, batch_num)\n",
        "        batch_num += 1\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward(torch.ones(loss.shape).cuda())\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        accf = acc.mean().item() #* gradient_accumulation_steps\n",
        "        lossf = loss.mean().item() * gradient_accumulation_steps\n",
        "\n",
        "        train_losses.append(lossf)\n",
        "        train_accs.append(accf)\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, acc {accf:.4f}, time {dt*1000:.2f}ms\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBkMH2gWl5co"
      },
      "source": [
        "# Final Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCmj4MBmAOjF"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/'\n",
        "\n",
        "best_val_loss = losses['val']\n",
        "best_val_acc = accs['val']\n",
        "if iter_num > 0:\n",
        "    checkpoint = {\n",
        "        'model': model.state_dict(),\n",
        "        'model_args': model_args,\n",
        "        'iter_num': iter_num,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'train/loss': losses['train'],\n",
        "        'val/loss': losses['val'],\n",
        "        'train/acc': accs['train'],\n",
        "        'val/acc': accs['val'],\n",
        "        'lr': learning_rate,\n",
        "        'config': config,\n",
        "    }\n",
        "    print(f\"saving checkpoint to {out_dir}\")\n",
        "    torch.save(checkpoint, os.path.join(out_dir, 'Pentagram_Music_Transformer_Small_Trained_Model_'+str(iter_num)+'_steps_'+str(round(best_val_loss.item(), 4))+'_loss_'+str(round(best_val_acc.item(), 4))+'_acc.pth'))\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwgV2ZA9ndQr"
      },
      "outputs": [],
      "source": [
        "data = [train_losses, train_accs, val_losses, val_accs]\n",
        "\n",
        "TMIDIX.Tegridy_Any_Pickle_File_Writer(data, '/content/losses_accuracies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vE5Z15fCz1M"
      },
      "outputs": [],
      "source": [
        "# Save training loss graph\n",
        "\n",
        "plt.plot([i for i in range(len(train_losses))] ,train_losses, 'b')\n",
        "plt.savefig('/content/training_loss_graph.png')\n",
        "plt.close()\n",
        "print('Done!')\n",
        "\n",
        "# Save training acc graph\n",
        "\n",
        "plt.plot([i for i in range(len(train_accs))] ,train_accs, 'b')\n",
        "plt.savefig('/content/training_acc_graph.png')\n",
        "plt.close()\n",
        "print('Done!')\n",
        "\n",
        "# Save validation loss graph\n",
        "\n",
        "plt.plot([i for i in range(len(val_losses))] ,val_losses, 'b')\n",
        "plt.savefig('/content/validation_loss_graph.png')\n",
        "plt.close()\n",
        "print('Done!')\n",
        "\n",
        "# Save validation acc graph\n",
        "\n",
        "plt.plot([i for i in range(len(val_accs))] ,val_accs, 'b')\n",
        "plt.savefig('/content/validation_acc_graph.png')\n",
        "plt.close()\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXay_Ed7mG5"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-juWMJ4JneZ8"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/OUT'\n",
        "# model init\n",
        "model_args = dict(n_layer=0, n_head=0, n_embd=0, block_size=0,\n",
        "                  bias=False, vocab_size=0, dropout=0, ignore_idx=1563) # start with model_args from command line\n",
        "\n",
        "\n",
        "print(f\"Restoring checkpoint from {out_dir}\")\n",
        "# resume training from a checkpoint.\n",
        "ckpt_path = os.path.join(out_dir, 'model_checkpoint.pth')\n",
        "checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
        "checkpoint_model_args = checkpoint['model_args']\n",
        "# force these config attributes to be equal otherwise we can't even resume training\n",
        "# the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "    model_args[k] = checkpoint_model_args[k]\n",
        "# create the model\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "# fix the keys of the state dictionary :(\n",
        "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "iter_num = checkpoint['iter_num']\n",
        "best_val_loss = checkpoint['best_val_loss']\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8u5TdxdnHDN"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "\n",
        "f = '/content/Pentagram-Music-Transformer-MI-Seed-1.mid'\n",
        "\n",
        "print('=' * 70)\n",
        "print('Pentagram Music Transformer Seed MIDI Loader')\n",
        "print('=' * 70)\n",
        "print('Loading seed MIDI...')\n",
        "print('=' * 70)\n",
        "print('File:', f)\n",
        "print('=' * 70)\n",
        "\n",
        "#=======================================================\n",
        "# START PROCESSING\n",
        "\n",
        "# Convering MIDI to ms score with MIDI.py module\n",
        "score = TMIDIX.midi2single_track_ms_score(open(f, 'rb').read(), recalculate_channels=False)\n",
        "\n",
        "# INSTRUMENTS CONVERSION CYCLE\n",
        "events_matrix = []\n",
        "itrack = 1\n",
        "patches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "while itrack < len(score):\n",
        "    for event in score[itrack]:\n",
        "        if event[0] == 'note' or event[0] == 'patch_change':\n",
        "            events_matrix.append(event)\n",
        "    itrack += 1\n",
        "\n",
        "events_matrix.sort(key=lambda x: x[1])\n",
        "\n",
        "events_matrix1 = []\n",
        "\n",
        "for event in events_matrix:\n",
        "        if event[0] == 'patch_change':\n",
        "              patches[event[2]] = event[3]\n",
        "\n",
        "        if event[0] == 'note':\n",
        "              event.extend([patches[event[3]]])\n",
        "\n",
        "              events_matrix1.append(event)\n",
        "\n",
        "if len(events_matrix1) > 0:\n",
        "  if min([e[1] for e in events_matrix1]) >= 0 and min([e[2] for e in events_matrix1]) >= 0:\n",
        "\n",
        "    #=======================================================\n",
        "    # PRE-PROCESSING\n",
        "\n",
        "    # checking number of instruments in a composition\n",
        "    instruments_list_without_drums = list(set([y[3] for y in events_matrix1 if y[3] != 9]))\n",
        "    instruments_list = list(set([y[3] for y in events_matrix1]))\n",
        "\n",
        "    if len(events_matrix1) > 0 and len(instruments_list_without_drums) > 0:\n",
        "\n",
        "      # recalculating timings\n",
        "      for e in events_matrix1:\n",
        "          e[1] = int(e[1] / 8) # Max 2 seconds for start-times\n",
        "          e[2] = int(e[2] / 16) # Max 4 seconds for durations\n",
        "\n",
        "      # Sorting by pitch, then by start-time\n",
        "      events_matrix1.sort(key=lambda x: x[4], reverse=True)\n",
        "      events_matrix1.sort(key=lambda x: x[1])\n",
        "\n",
        "      #=======================================================\n",
        "      # FINAL PRE-PROCESSING\n",
        "\n",
        "      melody_chords = []\n",
        "\n",
        "      pe = events_matrix1[0]\n",
        "\n",
        "      for e in events_matrix1:\n",
        "\n",
        "          # Cliping all values...\n",
        "          time = max(0, min(255, e[1]-pe[1]))\n",
        "          dur = max(0, min(255, e[2]))\n",
        "          cha = max(0, min(15, e[3]))\n",
        "          ptc = max(1, min(127, e[4]))\n",
        "\n",
        "          # Calculating octo-velocity\n",
        "          vel = max(1, min(127, e[5]))\n",
        "\n",
        "          pat = max(0, min(127, e[6]))\n",
        "\n",
        "          # Writing final note\n",
        "          melody_chords.append([time, dur, cha, ptc, vel, pat])\n",
        "\n",
        "          pe = e\n",
        "\n",
        "\n",
        "\n",
        "      #=======================================================\n",
        "      # FINAL PROCESSING\n",
        "      #=======================================================\n",
        "\n",
        "      melody_chords2 = []\n",
        "\n",
        "      # Break between compositions / Intro seq\n",
        "\n",
        "      if 9 in instruments_list:\n",
        "        drums_present = 1539 # Yes\n",
        "      else:\n",
        "        drums_present = 1538 # No\n",
        "\n",
        "      if melody_chords[0][2] != 9:\n",
        "          pat = melody_chords[0][5]\n",
        "      else:\n",
        "          pat = 128\n",
        "\n",
        "      melody_chords2.extend([1669, 1669, 1669, drums_present, 1540+pat])\n",
        "\n",
        "      #=======================================================\n",
        "\n",
        "      # TOTAL DICTIONARY SIZE 1669+1=1670\n",
        "\n",
        "      #=======================================================\n",
        "      # MAIN PROCESSING CYCLE\n",
        "      #=======================================================\n",
        "\n",
        "      note_counter = 1\n",
        "\n",
        "      for m in melody_chords:\n",
        "\n",
        "          if note_counter % 100 == 0:\n",
        "              nct = 1025+min(511, (note_counter // 100)) # note counter token\n",
        "              melody_chords2.extend([nct, nct, nct, nct, nct])\n",
        "\n",
        "          if m[2] != 9:\n",
        "              ptc = m[3]\n",
        "              pat = m[5]\n",
        "          else:\n",
        "              ptc = m[3] + 128\n",
        "              pat = 128\n",
        "\n",
        "          melody_chords2.extend([pat, m[0]+129, m[1]+385, ptc+641, m[4]+897])\n",
        "\n",
        "          note_counter += 1\n",
        "\n",
        "#======================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naf65RxUXwDg"
      },
      "outputs": [],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "#x = (torch.tensor(random.choice(train_data)[:1000], dtype=torch.long, device=device_type)[None, ...])\n",
        "#x = torch.tensor([[1669, 1669, 1669, 1538+1, 1540+40]], dtype=torch.long, device=device_type)\n",
        "x = torch.tensor([[1669, 1669, 1669]], dtype=torch.long, device=device_type)\n",
        "\n",
        "#x = torch.tensor([melody_chords2[:1000]], dtype=torch.long, device=device_type)\n",
        "\n",
        "\n",
        "# run generation\n",
        "#with torch.no_grad():\n",
        "with ctx:\n",
        "#        for k in range(1):\n",
        "  y = model.module.generate(x, 1000, temperature=0.85, return_prime=False)\n",
        "  #print(y[0].tolist())\n",
        "  print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smllKJg2td4i"
      },
      "outputs": [],
      "source": [
        "len(melody_chords2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlBzqWpAnZna"
      },
      "outputs": [],
      "source": [
        "#@title Test INTs\n",
        "\n",
        "data = y[0].tolist()\n",
        "\n",
        "print('Sample INTs', data[:15])\n",
        "\n",
        "out = data[:200000]\n",
        "\n",
        "if len(out) != 0:\n",
        "\n",
        "    song = out\n",
        "    song_f = []\n",
        "    time = 0\n",
        "    dur = 0\n",
        "    vel = 90\n",
        "    pitch = 0\n",
        "    channel = 0\n",
        "\n",
        "    patches = [0] * 16\n",
        "\n",
        "    channels = [0] * 16\n",
        "    channels[9] = 1\n",
        "\n",
        "\n",
        "    for ss in song:\n",
        "\n",
        "      if ss >= 0 and ss <= 128:\n",
        "\n",
        "        if ss < 128:\n",
        "\n",
        "            if ss not in patches:\n",
        "              cha = channels.index(0)\n",
        "              channels[cha] = 1\n",
        "\n",
        "              patches[cha] = ss\n",
        "              channel = patches.index(ss)\n",
        "            else:\n",
        "              channel = patches.index(ss)\n",
        "\n",
        "        if ss == 128:\n",
        "            channel = 9\n",
        "\n",
        "      if ss > 128 and ss <= 256:\n",
        "\n",
        "        time += (ss-129) * 8\n",
        "\n",
        "      if ss > 384 and ss <= 640:\n",
        "\n",
        "        dur = (ss-385) * 16\n",
        "\n",
        "      if ss > 640 and ss <= 896:\n",
        "\n",
        "        pitch = (ss-641) % 128\n",
        "\n",
        "      if ss > 896 and ss <= 1024:\n",
        "\n",
        "        vel = (ss-897)\n",
        "\n",
        "        song_f.append(['note', time, dur, channel, pitch, vel ])\n",
        "\n",
        "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
        "                                                        output_signature = 'Pentagram Music Transformer',\n",
        "                                                        output_file_name = '/content/Pentagram-Music-Transformer-Composition',\n",
        "                                                        track_name='Project Los Angeles',\n",
        "                                                        list_of_MIDI_patches=patches,\n",
        "                                                        number_of_ticks_per_quarter=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZGayVt6okU9"
      },
      "outputs": [],
      "source": [
        "patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al3TDlH7T8m7"
      },
      "outputs": [],
      "source": [
        "tok_emb = model.module.transformer.wte.weight.detach().cpu().tolist()\n",
        "\n",
        "cos_sim = metrics.pairwise_distances(\n",
        "  tok_emb, metric='cosine'\n",
        ")\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.imshow(cos_sim, cmap=\"inferno\", interpolation=\"nearest\")\n",
        "im_ratio = cos_sim.shape[0] / cos_sim.shape[1]\n",
        "plt.colorbar(fraction=0.046 * im_ratio, pad=0.04)\n",
        "plt.xlabel(\"Position\")\n",
        "plt.ylabel(\"Position\")\n",
        "plt.tight_layout()\n",
        "plt.plot()\n",
        "plt.savefig(\"/content/Pentagram-Music-Transformer-Tiny-Tokens-Embeddings-Plot.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z87TlDTVl5cp"
      },
      "source": [
        "# Congrats! You did it! :)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}